Project report: [pdf file](https://github.com/Phickies/GestureXR/blob/main/ProjectReport.pdf)

# Directory Introduction

This is the directory for the GestureXR, including sources code for the main hardware, AI model training, and building skeletons, project report and raw data.

# Project Introduction

The idea started with making a holographic tap track glove. The design is based on the most advanced Tap Strap 2 product for gesture keyboards (Tap strap 2 2024), the gesture is inspired by Apple Vision Pro Gestures, an advanced gesture recognition product that can recognize different gestures [2]. This interaction was inspired by the holographic table scene in the Iron Man movie[3] where the main character interacts with the holographic table to prototype his super armor by hand. Our goal is to replicate the most accurate interactions, just like in the movies. We designed a three-finger glove to interact with 3D objects in real-time through different IMUs (Inertial Measurement Units, a unit that measures body movement through acceleration and rotation) [1] attached to the fingertips. Users can grab, rotate, zoom in, zoom out, detach, and build objects with their hands or custom gestures to control virtual objects or extend use for any IoT devices. We built a prototype with 3 IMUs units attached to the finger of the user and a smaller version of the ESP32 S3 chip, which collected the data from the IMU and preprocessed it. By using a combination of a Long Short Term Memory deep learning model and a Convolutional Neural Network Model (CNN), we  train with the collected data and test it using scikit-learn model evaluation score metric. After training, the model is imported back into the ESP32 S3 chip to act as an edge device, allowing it to recognize gestures and interact with 3D objects in real time. For the visualizing and representing our product, we plan to use Unity to create a 3D space with floating materials and objects represented as a holographic interactive model. The predicted data we got from the ESP32 S3 chip will be sent to Unity using Wi-Fi.  The application for this project aims to create the first prototype for fast, visually real time prototyping for mechanical engineers or industrial designers who need to brainstorm, and real time prototyping within a team.
